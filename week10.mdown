##Learning With Large Datasets
現在では億単位の情報が手に入り、それを用いて機械学習の実装が可能である。線形回帰などの今まで扱ったアルゴリズムをそのまま用いると最急降下法の1ステップだけで億単位の総和を計算することになる。より計算量を減らせる方法があるのだがそもそもとして、億単位の情報が必要なのか、1000個のデータセットでは不十分なのかを吟味する必要がある。

そこでまず1000個のデータセットを使って学習曲線を書いてみる。
トレーニングセットの誤差とクロスバリデーションデータセットの誤差が同じ値に収束しそうであれば、underfittingの状態にあるので変数を増やしたりニューラルネットワークの隠れ層を増やしたりして対応するべきである。

##Stochastic Gradient Descent


##Mini-Batch Gradient Descent


##Stochatic Gradient Descent Convergence


##Online Learning


##Map Reduce and Data Parallelism