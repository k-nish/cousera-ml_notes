##Learning With Large Datasets
現在では億単位の情報が手に入り、それを用いて機械学習の実装が可能である。線形回帰などの今まで扱ったアルゴリズムをそのまま用いると最急降下法の1ステップだけで億単位の総和を計算することになる。より計算量を減らせる方法があるのだがそもそもとして、億単位の情報が必要なのか、1000個のデータセットでは不十分なのかを吟味する必要がある。

そこでまず1000個のデータセットを使って学習曲線を書いてみる。
トレーニングセットの誤差とクロスバリデーションデータセットの誤差が同じ値に収束しそうであれば、underfittingの状態にあるので変数を増やしたりニューラルネットワークの隠れ層を増やしたりして対応するべきである。

##Stochastic Gradient Descent
###確率的最急降下法
今までの計算よりも計算量を少なくする確率的最急降下法を説明する。

- これまでとの違い。
今まで用いてきたアルゴリズムのコスト関数を$J_\theta$とすると最急降下法は

octave:gradient descent

Repeat{
	$$\theta_j := \theta_j - \alpha \dfrac{1}{m} \sum_{i=1}^{m}(h_\theta(x^{(i)}) - y^{(i)})x_j^{(i)} = \theta_j - \alpha \dfrac{1}{m} \sum_{i=1}^{m}\dfrac{\partial J}{\partial \theta_j}$$
} (for every j = 0,...,n)


これを以下の計算で代用する

octave:Stochastic Gradient Descent

Repeat{<br>
	for i = 1,...,m{
		$$\theta_j := \theta_j - \alpha \dfrac{1}{m} \dfrac{\partial J}{\partial \theta_j}$$
	}(for j = 0,...,n)

}

こうすることによって計算量はかなり少なくなる。
しかし、このアルゴリズムではそれぞれのデータセット近づくように$\theta$を最適化するため、具体的な値には収束せず、また収束には時間がかかることがある。ある値に収束するわわけではないものの、ある値の領域にとどまるようになるのでそれで十分となるケースが多い。それぞれのデータセットに近づくように$\theta$が変化していくので上のアルゴリズムを動かす前にデータセットをランダムに並び替える必要がある。

##Mini-Batch Gradient Descent
$\theta$を計算するとき、
バッチアルゴリズムではデータセット全ての値を参照していたが<br>
確率的最急勾配法では1件のみの値を参照していた。<br>
ミニバッチアルゴリズムではb個のデータセットを参照する。bは10などの値(2~100でok)が入る。

###確率的最急降下法とどちらがよいのか
・ミニバッチアルゴリズムの利点
ベクトル化できれば並列計算が可能となり確率的最急降下法よりも計算が早く終わる可能背性がある

・ミニバッチアルゴリズムの欠点
bというパラメータが増えたので計算が遅くなる可能性がある

##Stochatic Gradient Descent Convergence


##Online Learning


##Map Reduce and Data Parallelism