##Learning With Large Datasets
現在では億単位の情報が手に入り、それを用いて機械学習の実装が可能である。線形回帰などの今まで扱ったアルゴリズムをそのまま用いると最急降下法の1ステップだけで億単位の総和を計算することになる。より計算量を減らせる方法があるのだがそもそもとして、億単位の情報が必要なのか、1000個のデータセットでは不十分なのかを吟味する必要がある。

そこでまず1000個のデータセットを使って学習曲線を書いてみる。
トレーニングセットの誤差とクロスバリデーションデータセットの誤差が同じ値に収束しそうであれば、underfittingの状態にあるので変数を増やしたりニューラルネットワークの隠れ層を増やしたりして対応するべきである。

##Stochastic Gradient Descent
###確率的最急降下法
今までの計算よりも計算量を少なくする確率的最急降下法を説明する。

- これまでとの違い。
今まで用いてきたアルゴリズムのコスト関数を$J_\theta$とすると最急降下法は

octave:gradient descent

Repeat{
	$$\theta_j := \theta_j - \alpha \dfrac{1}{m} \sum_{i=1}^{m}(h_\theta(x^{(i)}) - y^{(i)})x_j^{(i)} = \theta_j - \alpha \dfrac{1}{m} \sum_{i=1}^{m}\dfrac{\partial J}{\partial \theta_j}$$
} (for every j = 0,...,n)


これを以下の計算で代用する

octave:Stochastic Gradient Descent

Repeat{<br>
	for i = 1,...,m{
		$$\theta_j := \theta_j - \alpha \dfrac{1}{m} \dfrac{\partial J}{\partial \theta_j}$$
	}(for j = 0,...,n)

}

こうすることによって計算量はかなり少なくなる。
しかし、このアルゴリズムではそれぞれのデータセット近づくように$\theta$を最適化するため、具体的な値には収束せず、また収束には時間がかかることがある。ある値に収束するわわけではないものの、ある値の領域にとどまるようになるのでそれで十分となるケースが多い。それぞれのデータセットに近づくように$\theta$が変化していくので上のアルゴリズムを動かす前にデータセットをランダムに並び替える必要がある。

##Mini-Batch Gradient Descent
$\theta$を計算するとき、
バッチアルゴリズムではデータセット全ての値を参照していたが<br>
確率的最急勾配法では1件のみの値を参照していた。<br>
ミニバッチアルゴリズムではb個のデータセットを参照する。bは10などの値(2~100でok)が入る。

###確率的最急降下法とどちらがよいのか
・ミニバッチアルゴリズムの利点
ベクトル化できれば並列計算が可能となり確率的最急降下法よりも計算が早く終わる可能背性がある

・ミニバッチアルゴリズムの欠点
bというパラメータが増えたので計算が遅くなる可能性がある

##Stochastic Gradient Descent Convergence
##収束を確認する方法と学習率を設定する方法
確率的最急降下法のアルゴリズムを走らせている間に、(x(i),y(i))から得られたデータを用いて(x(i+1),y(i+1))を計算し誤差を測るといい。実際の計算としては1000回の試行ごとにコスト関数をプロットしてみることになる。

確率的最急降下法ではある程度の幅で振動し続けるので全体的な動向が見えにくくなる。あまり収束していなかったり、全く変化していなかったら5000回ごとのコスト関数をプロットし、もしコスト関数が発散しているようであれば学習率を小さくしてみるといい。

学習率は反復回数が大きくなるにつれて小さくなる方がコスト関数の振動の幅が小さくなり、よりよいアルゴリズムとなる。しかし、アルゴリズムを走らせている間に学習率を計算し直すことになり、手間が増えることにもなる。

##Online Learning
オンライン学習とは、webサイトから得られた情報といった次々にくる情報からユーザーの希望などを学習するためのアルゴリズムである。

このアルゴリズムにおいてはデータセットとしてデータを保存することなく、1回データを利用して$\theta$を最適化したらまた新たなデータを使うことにある。
従ってユーザーの嗜好が変化したらそれに合わせて$\theta$も最適される。
アルゴリズムとしては確率的最急降下法と同様になる。

##Map Reduce and Data Parallelism