##Optimization Objective
###alternative view of logistic regression
SVM(サポートベクトルマシン)というよりよいアルゴリズムの目的関数を考える。
まず、論理回帰の場合の処理を思い出してみる。
$$h_{\theta}(x) = \dfrac{1}{1 + e^{-\theta^{T}x}}$$
と仮定した時に
y = 1ならば$h_{\theta}(x)\approx 1$かつ $\theta^{T}x \gg 0$となってほしく、<br>
y = 0ならば$h_{\theta}(x)\approx 0$かつ $\theta^{T}x \ll 0$となってほしい。<br>
ここでy=1,y=0の場合のsigmoid関数をそれぞれ2本の直線で近似することを考える。<br>
すなわち、$-logh_{\theta}(x^{(i)})$を2本の直線からなる$cost_1(\theta^{T}x^{(i)})$で近似し、同様に$-log(1-h_{\theta}(x^{(i)}))$で近似する。

また論理回帰では正規化項に重みをつけていたが、SVMでは逆の項に重みをつける。<br>
従って論理回帰の目的関数が
$$\stackrel{min}{\theta} \dfrac{1}{m}[\sum_{i=1}^{m}y^{(i)}(-logh_{\theta}(x^{(i)}) + (1 - y^{(i)})(-log(1-h_{\theta}(x^{(i)})))] + \dfrac{\lambda}{2m} \sum_{j = 1}^{n}\theta_j^2$$
となり、SVMの目的関数が
$$\stackrel{min}{\theta} C[\sum_{i=1}^{m}y^{(i)}cost_1(\theta^{T}x^{(i)}) + (1 - y^{(i)})cost_0(\theta^{T}x^{(i)})] + \dfrac{1}{2} \sum_{j = 1}^{n}\theta_j^2$$
となる。この定数の置き方においてCと$\lambda$は逆数の関係にある。

SVMは確率を計算するのではなく、目的関数を最小化する$\theta$をもとめ、かつy=0か1の予想までする。

##Large Margin Intuition
「SVMは大きなマージンを持つ分類器」という命題を説明する。<br>
説明しやすくするためにCが十分大きいという条件で先の目的関数の最小値問題を考える。<br>
前回の授業で行った近似を用いると以下の最小値問題を考えることと同値になる。

$$\stackrel{min}{\theta}\sum_{j=1}^{n}\theta_{j}^2$$
条件
$\begin{cases}
\theta^Tx \ge 1 & \text{y=1のとき}\\
\theta^Tx \le -1 & \text{y=0のとき}
\end{cases}$

この条件でSVMを実行するとデータセットからの距離(定義が不明)が最も大きい境界を得ることができる。

##Mathematics Behind Large Margin Classification
SVMの裏にある数学

###ベクトルの内積とは
$\[ u = \left(
\begin{array}{c}
u_1 \\
u_2
\end{array}
\right) \]$

$\[ v = \left(
\begin{array}{c}
v_1 \\
v_2
\end{array}
\right) \]$

に対して$u^Tv$を考える。
vのuへの正射影ベクトルの長さをp(符号付長さ)とすると
$$u^Tv = p・\|u\|=u_1v_1+u_2v_2$$

SVMでは以下の条件で$\theta^Tp^{(i)}$の最小値問題を考えることになる。
$\begin{cases}
\|\theta\| ・p^{(i)} \ge 1 & \text{y=1のとき}\\
\|\theta\| ・p~{(i)} \le -1& \text{y=0のとき}
\end{cases}$

$p^{(i)}$は$\theta$方向へのそれぞれのデータセット(ベクトル)の正射影の長さである。

##Kernel Ⅰ
例えばデータセットの変数が$x_1,x_2$しかなくても分析に必要な関数に$x_1x_2$や$x_{1}^{3}x_2$が必要になるときもある。<br>
どのように変数を選べばいいのかを今回考える。<br>
変数が$x_1,x_2$しかない場合を考える。<br>
分析に用いるの関数を$\theta_0+\theta_1f_1+\theta_2f_2+\theta_3f_3+\cdots$とする。fには$x_1$か$x_2$からなる関数が含まれる。この関数が0より大きければy=1と予想し、0より小さければy=0と予想する。

$x_1,x_2$を座標軸にもつ座標平面上で任意の点を手動で選ぶ(ランドマークという)[選び方は次以降の授業で]。選んだランドマークを用いて以下のようにカーネル(これはガウスカーネル)を定義する。$f_iはxとl_i$がどれだけ近いか示している。与えられたxに対して<br>
$f_i = similarity(x,l^{(i)})=\exp(-\dfrac{\|x-l^{(i)}\|^2}{2\sigma^2}) = \exp(\dfrac{\sum_{j=1}^{n}\|x_j-l^{(i)}\|^2}{2\sigma^2})$

##Kernel Ⅱ
具体例を用いてカーネルを用いたSVMの実装を考える。
データセットがm個あるとき、その全てをランドマークとしてカーネルを考えるべきであり、$f \in \mathbb{R}^{m+1},\theta \in \mathbb{R}^{m+1}$となる。<br>
すなわち、$\theta^Tf = \theta_0f_0 + \theta_1f_1 + \theta_2f_2 + \cdots \ge 0 \Rightarrow y=1$ となる。

SVMでは以下の式の最小値問題になる。
$\stackrel{min}{\theta}C \sum_{i=1}^{m} y^{(i)}cost_1(\theta^Tf^{(i)})+(1-y^{(i)})cost_0(\theta^Tf^{(i)})+ \dfrac{1}{2} \sum_{j=1}^{m}\theta_j^2$

正規化のところでも書かれた通り、正規化項で$\theta_0$は計算に含まない。

また、SVMを計算する実際のソフトでは正確には$\theta^2 = \theta^T\theta$を計算しているわけではなく、カーネルに由来するMを用いて$\theta^TM\theta$を計算している。

###定数Cと$\sigma^2$の選び方
定数Cについて<br>
Cが大きい時($\lambda$が小さい時) lower bias, high variance →overfitしやすい<br>
Cが小さい時($\lambda$が大きい時) high bias, lower variance →underfitしやすい<br>

$\sigma^2$について<br>
$\sigma^2$が大きい時$f_i$がゆっくりと変化するのでランドマークに近いカーネルの値が大きくなりにくい。→higher bias, lower variance<br>
$\sigma^2$が小さい時$f_i$が早く変化するのでランドマークから少しでも離れるとカーネルの値が大きくなる。→lower bias, higher variance

##Using A SVM

