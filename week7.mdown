##Optimization Objective
###alternative view of logistic regression
SVM(サポートベクトルマシン)というよりよいアルゴリズムの目的関数を考える。
まず、論理回帰の場合の処理を思い出してみる。
$$h_{\theta}(x) = \dfrac{1}{1 + e^{-\theta^{T}x}}$$
と仮定した時に
y = 1ならば$h_{\theta}(x)\approx 1$かつ $\theta^{T}x \gg 0$となってほしく、<br>
y = 0ならば$h_{\theta}(x)\approx 0$かつ $\theta^{T}x \ll 0$となってほしい。<br>
ここでy=1,y=0の場合のsigmoid関数をそれぞれ2本の直線で近似することを考える。<br>
すなわち、$-logh_{\theta}(x^{(i)})$を2本の直線からなる$cost_1(\theta^{T}x^{(i)})$で近似し、同様に$-log(1-h_{\theta}(x^{(i)}))$で近似する。

また論理回帰では正規化項に重みをつけていたが、SVMでは逆の項に重みをつける。<br>
従って論理回帰の目的関数が
$$\stackrel{min}{\theta} \dfrac{1}{m}[\sum_{i=1}^{m}y^{(i)}(-logh_{\theta}(x^{(i)}) + (1 - y^{(i)})(-log(1-h_{\theta}(x^{(i)})))] + \dfrac{\lambda}{2m} \sum_{j = 1}^{n}\theta_j^2$$
となり、SVMの目的関数が
$$\stackrel{min}{\theta} C[\sum_{i=1}^{m}y^{(i)}cost_1(\theta^{T}x^{(i)}) + (1 - y^{(i)})cost_0(\theta^{T}x^{(i)})] + \dfrac{1}{2} \sum_{j = 1}^{n}\theta_j^2$$
となる。この定数の置き方においてCと$\lambda$は逆数の関係にある。

SVMは確率を計算するのではなく、目的関数を最小化する$\theta$をもとめ、かつy=0か1の予想までする。

##Large Margin Intuition
「SVMは大きなマージンを持つ分類器」という命題を説明する。<br>
説明しやすくするためにCが十分大きいという条件で先の目的関数の最小値問題を考える。<br>
前回の授業で行った近似を用いると以下の最小値問題を考えることと同値になる。

$$\stackrel{min}{\theta}\sum_{j=1}^{n}\theta_{j}^2$$
条件
$\begin{cases}
\theta^Tx \ge 1 & \text{y=1のとき}\\
\theta^Tx \le -1 & \text{y=0のとき}
\end{cases}$

この条件でSVMを実行するとデータセットからの距離(定義が不明)が最も大きい境界を得ることができる。

##Mathematics Behind Large Margin Classification
SVMの裏にある数学

###ベクトルの内積とは
$\[ u = \left(
\begin{array}{c}
u_1 \\
u_2
\end{array}
\right) \]$

$\[ v = \left(
\begin{array}{c}
v_1 \\
v_2
\end{array}
\right) \]$

に対して$u^Tv$を考える。
vのuへの正射影ベクトルの長さをp(符号付長さ)とすると
$$u^Tv = p・\|u\|=u_1v_1+u_2v_2$$

SVMでは以下の条件で$\theta^Tp^{(i)}$の最小値問題を考えることになる。
$\begin{cases}
\|\theta\| ・p^{(i)} \ge 1 & \text{y=1のとき}\\
\|\theta\| ・p~{(i)} \le -1& \text{y=0のとき}
\end{cases}$

$p^{(i)}$は$\theta$方向へのそれぞれのデータセット(ベクトル)の正射影の長さである。

##Kernel Ⅰ
例えばデータセットの変数が$x_1,x_2$しかなくても分析に必要な関数に$x_1x_2$や$x_{1}^{3}x_2$が必要になるときもある。<br>
どのように変数を選べばいいのかを今回考える。<br>
変数が$x_1,x_2$しかない場合を考える。<br>
分析に用いるの関数を$\theta_0+\theta_1f_1+\theta_2f_2+\theta_3f_3+\cdots$とする。fには$x_1$か$x_2$からなる関数が含まれる。この関数が0より大きければy=1と予想し、0より小さければy=0と予想する。

$x_1,x_2$を座標軸にもつ座標平面上で任意の点を手動で選ぶ(ランドマークという)[選び方は次以降の授業で]。選んだランドマークを用いて以下のようにカーネル(これはガウスカーネル)を定義する。$f_iはxとl_i$がどれだけ近いか示している。与えられたxに対して<br>
$f_i = similarity(x,l^{(i)})=\exp(-\dfrac{\|x-l^{(i)}\|^2}{2\sigma^2}) = \exp(\dfrac{\sum_{j=1}^{n}\|x_j-l^{(i)}\|^2}{2\sigma^2})$

##Kernel Ⅱ
具体例を用いてカーネルを用いたSVMの実装を考える。
データセットがm個あるとき、その全てをランドマークとしてカーネルを考えるべきであり、$f \in \mathbb{R}^{m+1},\theta \in \mathbb{R}^{m+1}$となる。<br>
すなわち、$\theta^Tf = \theta_0f_0 + \theta_1f_1 + \theta_2f_2 + \cdots \ge 0 \Rightarrow y=1$ となる。

SVMでは以下の式の最小値問題になる。
$\stackrel{min}{\theta}C \sum_{i=1}^{m} y^{(i)}cost_1(\theta^Tf^{(i)})+(1-y^{(i)})cost_0(\theta^Tf^{(i)})+ \dfrac{1}{2} \sum_{j=1}^{m}\theta_j^2$

正規化のところでも書かれた通り、正規化項で$\theta_0$は計算に含まない。

また、SVMを計算する実際のソフトでは正確には$\theta^2 = \theta^T\theta$を計算しているわけではなく、カーネルに由来するMを用いて$\theta^TM\theta$を計算している。

###定数Cと$\sigma^2$の選び方
定数Cについて<br>
Cが大きい時($\lambda$が小さい時) lower bias, high variance →overfitしやすい<br>
Cが小さい時($\lambda$が大きい時) high bias, lower variance →underfitしやすい<br>

$\sigma^2$について<br>
$\sigma^2$が大きい時$f_i$がゆっくりと変化するのでランドマークに近いカーネルの値が大きくなりにくい。→higher bias, lower variance<br>
$\sigma^2$が小さい時$f_i$が早く変化するのでランドマークから少しでも離れるとカーネルの値が大きくなる。→lower bias, higher variance

##Using A SVM
###SVM実装するときにしなくてはいけないこと
最適化問題をとくライブラリ(liblinear,libsvmなど)を用いることになる。

- 必要なのは定数Cを決めること。
- カーネルを使うかどうかを決めること
	- カーネルを使わない(linear kernel)で実装する場合(変数が多く、データセットはそこまで多くない場合)は何も設定する値は存在しない。
	- カーネルを使うとき(変数は少ないものの、データセットは多い場合)$\sigma^2$を決める必要がある

時にはカーネル(ガウスカーネル)を計算するコードを書く必要がある。そのときに変数のスケーリングをしなくてはならない。
ex.住宅の価格予想で敷地面積の項と部屋数の項をそのまま計算に用いると、敷地面積の項の寄与が非常に大きくなりいい予測ができなくなってしまう。

###その他のカーネル
全てのカーネルはmarcerの定理を満たさないといけないことになっている。満たしていないとSVMがうまく動かなくなってしまう。

ガウスカーネル以外のカーネルを紹介する

- 多項式カーネル
	- similarity(x,l) = $(X^Tl + C_1)^{C_2}$で表され、パラメータは$C_1とC_2$の二つになる。
- String kernel
	- 文字列の類似性を測るために用いる。
- chi-square kernel
- histogram intersection kernel

###multi-class classfication
多くのSVMマシンではすでにマルチクラスの分類機能が実装されているはずだが、もし実装されていない場合onevsALLのときのようにして実装すればよい

####どのアルゴリズムを使うか

- 変数が少なく、データセットの数が中程度のとき(変数の個数が1-1000,データセット数が10-50000くらい)
	- ガウスカーネルを用いたSVMを使うべき
- 変数の少なく、データセット数が多いとき(変数の個数が1~1000,データセット数が50000以上)
	- 変数の数を増やしてから論理回帰を使う(データセット数が増えて行くにつれてSVMが遅くなっていく。)
-ニューラルネットワークは、ほとんどの場合でうまく行くが学習に時間がかかってしまう


