##Deciding what to next
より正確なアルゴリズムをつくるために<br>
-より多くのデータセットを用いる<br>
-featuresを増減させる<br>
-polynomial features$(ex.x_1x_2)$を増やす<br>
-λを増減させる<br>

##evaluating a hypothesis
与えられたデータセットをトレーニングセット(70%)とテストセット(30%)に分ける

データセットから得られるコスト関数$J(\Theta)$を求める

このコスト関数$J(\Theta)$にテストセットのデータを加え、誤差を計算する
$$J_{test}(\Theta) = -\dfrac{1}{m_{test}}\sum_{i=1}^{m_{test}}y_{test}^{(i)}logh_{\theta}(x_{test}^{(i)})+(1-y_{test}^{(i)})logh_{\theta}(x_{test}^{(i)})$$

-Misclassification error(0/1misclassification error)<br>
err($h_{\theta}(x)$,y) = $\begin{cases}
1 & \text{$h_{\theta}(x) \ge0.5$,y=0 or $h_{\theta}(x)$<0.5,y=1のとき　}\\
0 & \text{それ以外のとき}
\end{cases}$

##Model Selection and train/validation/test sets
よりよいモデル(具体的には、コスト関数の次数)を見つける手段
データセットをtraining set/cross validation set/test setに分ける。training setで一般化されたコスト関数を用いて、cross validation setのコスト関数を考える

$$J_{train}(\theta) = \dfrac{1}{2m}\sum_{i=1}^{m}(h_{\theta}(x^{(i)})-y^{(i)})^{2}$$

$$J_{cv}(\theta) = \dfrac{1}{2m_{cv}}\sum_{i=1}^{m_{cv}}(h_{\theta}(x_{cv}^{(i)})-y_{cv}^{(i)})^{2}$$

##diagnosing Bias.vs Variance
アルゴリズムがオーバーフィットしているのか、アンダーフィットしているのか。

Bias(underfit)<br>
 $J_{train}(\theta)$ will be high & $J_{cv} \approx J_{train}$

Variance(overfit)<br>
 $J_{train}(\theta)$ will be low & $J_{cv}(\theta)\gg J_{train}(\theta)$

##Regularization and Bias/Variance
正規化係数の選び方(how to choose regularization parameter)
$$h_{\theta}(x) = \theta_{0}+\theta_{1}x+\theta_{2}x^2+\theta_{3}x^3+\theta_{4}x^4$$
$$J(\theta) = \dfrac{1}{2m}\sum_{i=1}^{m}(h_{\theta}(x^{(i)})-y^{(i)})^{2}+ \dfrac{\lambda}{2m}\sum_{j=1}^{m}\theta_{j}^{2}$$

この$\lambda$に0,0.01,0.02,0.04,...,10.24と値を代入し、それぞれの$\theta$でJ($\theta$)が最小となる$\theta$をそれぞれ$\theta^{1}$,$\theta^{2}$,...,$\theta^{12}$としていく。もっとも$J_{cv}(\theta)$が小さくなる時の次数がモデルとして適切な次数と考えられる

##Learning Curves
-if a learning algorithm is suffering from high bias, getting more training data will not (by itself) new much.

-if a learning algorithm is suffering from high variance, getting more training data is likely to help.

-learing curves
縦軸:error(予測値と実際値の誤差の二乗和)
横軸:m(training set size)

##Deciding What to Do Next Revisited
-よりアルゴリズムを作るために

1. より多くのデータセットを用いる→for high variance
2. 変数の数を減らす→for high variance
3. 変数の数を増やす→for high bias
4. 多項式の変数を増やす→for high bias
5. λを小さくする→for high bias
6. λを大きくする→for high variance

####Neural networks and overfitting
小さい(レイヤーの数や変数の数が小さい)ニューラルネットワークでは計算は速いが、underfittingになりやすい。
それに対して、大きいニューラルネットワークでは計算が遅くなる可能性はあるもののunderfittingになることはなく、うまく正規化することでoverfittingには対処できる。

従って大きいニューラルネットワークの方がよいモデルである可能性が高い。また、隠れレイヤーの数は1が基本である。

##Prioritizing what to work on
機械学習のシステム設計

###スパム分類器の例
building a spam classfier

- how to spend your time to make it have low error?
	- Collect lots of data
	- Develop sophisticated features based on email routing information
	- Develop sophisticated features for message body(単数形と複数形,大文字と小文字の違い)
	- Develop sophisticated algorithm to detect misspellings

##Error analysis
Recommended approach

- start with a simple algorithm that you can implement quickly. Implement it and test it on your cross-validation data.
- plot learning curves to decide if more data, more features, etc. are likely to help.
- error analysis: manually examint the examples (in cross validation set) that your algorithm made errors on. See if you spot any systematic trend in what type of examples it is making errors on.

###The importance of numerical evaluation
各分詞形による変化など同様の単語をそう分類するべきか。"Porter Stemmer"などの"stemming"アルゴリズムを使うことが一つの解決法になる。
stemming アルゴリズムが有用かどうかを判断するためにはそれらを実装した場合とそうでない場合でデータセットを用いて評価すればよい。(クロスバリデーションセットを用いることが正しい評価方法である。)

##Error Metrics for Skewed Classes

##Trading Off Precision and Recall

##Data for machine learning

